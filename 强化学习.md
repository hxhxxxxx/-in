第一课
例子：grid world。网格世界，一个机器人去走
概念
**state**  在例子中，状态就是不同的位置(x,y)
**state space**     所有状态的一个集合
**action**    在每个状态可以采取的行动，在例子里有五个行动，上下左右，以及不动
**action space**    所有行动的集合，依赖于状态，每个状态的行动都不一样
**state transition**    通过某个行动状态发生改变，定义了agent跟环境的交互 可以用概率的方法来描述
**forbidden area**     1.可以进入但是会受到惩罚 2.不能进入 在本次学习中都考虑第一种方法，这种方法更一般化但是也更加困难
**policy**  一种策略，仍用条件概率   pai(a1|s1)=0 pai(a2|s1)=1,表示这个策略在状态为s1的情况都采用a2,这种是确定性的情况，也可以是不确定性的情况

**reward**  用一个具体的数字来表示做出某个动作的成本，用正数代表奖励，负数代表惩罚，这样可以去人为的引导来实现目标

**trajectory**一个状态-动作-奖励链
**return**经过一系列的trajectory的所以reward总和，它的作用是从数学的意义上来描述策略的好坏
**discounted return** 结合一个discount rate来防止无穷的状态，通过控制rate值去平衡未来reward的远近，近视或远视
**episode** 通常是有限步的

**MDP** 
要素1:集合：state action reward
要素2:probability distribution:状态转移概率  reward的概率
要素3:policy:采取某种行动的概率
要素4:马尔可夫性质：状态的转移以及奖励的概率 跟历史没有关系
所以MDP：M代表马尔可夫性质；Decision代表策略policy;process代表从什么状态到什么状态，由前两种要素来描述

第二课：贝尔曼公式

不同状态出发得到的return跟别的状态的return有关，得到的就是贝尔曼公式
bootstrapping，从自己出发不断迭代得到自己

   At
St--->Rt+1,St+1
St-->At:由策略决定
St,At-->Rt+1:由reward transition probability决定 
St,At-->St+1:由state transition probability决定

多步就成了trajectory
  At           At+1         At+2
St--->Rt+1,St+1--->Rt+2,St+2--->Rt+3 ····
由这个得到的discounted return 
Gt=Rt+1 + &Rt+2 +&^2*Rt+3+·····

state value:就是G的期望值，是以以s为出发，基于某个策略，不单单是一个数值，也可以看作一个价值
跟return的区别 return是针对单个trajectory求的return 而state value是多个trajectory得到的return再求平均值



10月11号 sarsa算法。 off policy   Q-learnin















